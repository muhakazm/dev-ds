{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0f2ac7-f47f-42a5-9117-accab771e93b",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "**About**\n",
    "- Task type: classification, regression\n",
    "- How it works: Looks at the 'k' closest data points (neighbors) to a new input and predicts based on the majority class among them.\n",
    "- Intuition: \"Tell me who your neighbors are, and I’ll tell you who you are.\"\n",
    "- Pros: Simple, no training phase.\n",
    "- Cons: Slow with large datasets, sensitive to irrelevant features.\n",
    "\n",
    "**When you want to classify a new data point, the algorithm**\n",
    "- Looks for the `k` closest data points (neighbors) to that point in the dataset — based on distance (like Euclidean distance).\n",
    "- Then, it checks which class (label) is the most common among those 10 neighbors.\n",
    "- It assigns the most frequent class to the new point.\n",
    "- So, the number `k` refers to how many neighbors are considered during prediction\n",
    "\n",
    "**Regression note**\n",
    "- KNN is a non-parametric model — it doesn't try to fit a global line or curve (like in linear regression). Instead, it makes predictions locally by just looking at the nearby data points.\n",
    "- Finds the k nearest neighbors in the training data.\n",
    "- Takes the average of their y-values.\n",
    "- Returns that average as the prediction for x.\n",
    "\n",
    "**How distance works**\n",
    "- KNN is based on distance (like Euclidean, Manhattan, or even cosine similarity).\n",
    "- Choosing the right distance metric can really affect performance.\n",
    "\n",
    "**Choosing the right value of k**\n",
    "- Small `k`: sensitive to noise.\n",
    "- Large `k`: might blur class boundaries.\n",
    "- You can use cross-validation to find the best `k`.\n",
    "\n",
    "**Cross-Validation**\n",
    "- Cross-validation is a technique used to evaluate how well a machine learning model generalizes to unseen data — especially useful for picking the best `k` in KNN.\n",
    "\n",
    "**Most common cross-validation type: `k`-fold cross-validation**\n",
    "- Split your training data into `k` equal parts (e.g., 5 or 10).\n",
    "- Use `k–1` folds to train and 1 fold to validate the model.\n",
    "- Rotate through all the folds so each fold gets to be the validation set once.\n",
    "- Average the performance across all folds.\n",
    "\n",
    "**Feature scaling**\n",
    "- KNN is sensitive to scale — features like age (1–100) and income (thousands) can skew distance.\n",
    "- Normalization or standardization is usually needed.\n",
    "\n",
    "**Handling ties**\n",
    "- A tie happens during classification when there’s an equal number of votes for two (or more) classes among the `k` neighbors.\n",
    "- What if in `k=4`, two classes get two votes each?\n",
    "- Choose randomly (not ideal).\n",
    "- Use weighted voting (closer neighbors have more influence).\n",
    "- Pick the class of the closest neighbor.\n",
    "- Use an odd number for `k` to avoid ties in binary classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python:ds-gen",
   "language": "python",
   "name": "ds-gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
